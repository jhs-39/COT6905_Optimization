{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd594df",
   "metadata": {},
   "source": [
    "# Week 8: SVD, Linear Transform, Compression\n",
    "\n",
    "## Lecture 29: Singular value decomposition\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "    src=\"https://www.youtube.com/embed/TX_vooSnhm8\"\n",
    "    frameborder=\"0\"\n",
    "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "\n",
    "The **singular value decomposition** (SVD) is the ultimate factorization: it works for **any matrix** (square or rectangular, full rank or not) and reveals its **rank**, **fundamental subspaces**, and **condition number** (read more on this independently of lecture)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. SVD Form\n",
    "\n",
    "For any $m \\times n$ matrix $A$:  \n",
    "\\[\n",
    "\\boxed{A = U \\Sigma V^T}\n",
    "\\]\n",
    "- $U$: $m \\times m$ orthogonal ($U^T U = I$)  \n",
    "- $\\Sigma$: $m \\times n$ diagonal (singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq 0$)  \n",
    "- $V$: $n \\times n$ orthogonal ($V^T V = I$)\n",
    "\n",
    "The **rank** $r = \\#$ of nonzero $\\sigma_i$.\n",
    "\n",
    "> **Note**: If $A$ is symmetric positive definite, SVD = eigenvalue decomposition: $A = Q \\Lambda Q^T$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Geometric Interpretation\n",
    "\n",
    "- Columns of $V$ = **orthonormal basis** for row space $C(A^T)$ (input space).  \n",
    "- Columns of $U$ = **orthonormal basis** for column space $C(A)$ (output space).  \n",
    "- $A$ maps $v_i$ to $\\sigma_i u_i$:  \n",
    "  \\[\n",
    "  \\boxed{A v_i = \\sigma_i u_i}\n",
    "  \\]\n",
    "- For $i > r$, $\\sigma_i = 0$ → $v_i$ in null space $N(A)$.\n",
    "\n",
    "Full matrix equation:  \n",
    "\\[\n",
    "A V = U \\Sigma \\quad \\Rightarrow \\quad A = U \\Sigma V^T\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Computing SVD: Via Eigenvalues of $A^T A$ and $A A^T$\n",
    "\n",
    "\\[\n",
    "A^T A = (U \\Sigma V^T)^T (U \\Sigma V^T) = V \\Sigma^T U^T U \\Sigma V^T = V (\\Sigma^T \\Sigma) V^T\n",
    "\\]\n",
    "\n",
    "- $\\Sigma^T \\Sigma$ = diagonal with $\\sigma_i^2$ → eigenvalues of $A^T A$.  \n",
    "- $V$ = eigenvectors of $A^T A$.\n",
    "\n",
    "Similarly:  \n",
    "\\[\n",
    "A A^T = U (\\Sigma \\Sigma^T) U^T\n",
    "\\]\n",
    "- $U$ = eigenvectors of $A A^T$.\n",
    "\n",
    "> **Key**: Eigenvalues of $A^T A$ and $A A^T$ are the same (nonzero) $\\sigma_i^2$.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Example: Full Rank Matrix\n",
    "\n",
    "\\[\n",
    "A = \\begin{pmatrix} 4 & 4 \\\\ -3 & 3 \\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "First, $A^T A = \\begin{pmatrix} 25 & 7 \\\\ 7 & 25 \\end{pmatrix}$\n",
    "\n",
    "Eigenvalues: $\\lambda_1 = 32$, $\\lambda_2 = 18$ → $\\sigma_1 = \\sqrt{32} = 4\\sqrt{2}$, $\\sigma_2 = \\sqrt{18} = 3\\sqrt{2}$\n",
    "\n",
    "Eigenvectors (normalized):  \n",
    "$v_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $v_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n",
    "\n",
    "Now, $A A^T = \\begin{pmatrix} 32 & 0 \\\\ 0 & 18 \\end{pmatrix}$\n",
    "\n",
    "Eigenvectors (normalized):  \n",
    "$u_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $u_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "**Note**: Signs may vary; adjust to match $A v_i = \\sigma_i u_i$.\n",
    "\n",
    "SVD:  \n",
    "\\[\n",
    "A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 4\\sqrt{2} & 0 \\\\ 0 & 3\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\end{pmatrix}^T\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Example: Rank-Deficient Matrix\n",
    "\n",
    "\\[\n",
    "A = \\begin{pmatrix} 4 & 3 \\\\ 8 & 6 \\end{pmatrix} \\quad (\\text{rank 1})\n",
    "\\]\n",
    "\n",
    "$A^T A = \\begin{pmatrix} 80 & 60 \\\\ 60 & 45 \\end{pmatrix}$\n",
    "\n",
    "Eigenvalues: $\\lambda_1 = 125$, $\\lambda_2 = 0$ → $\\sigma_1 = \\sqrt{125} = 5\\sqrt{5}$, $\\sigma_2 = 0$\n",
    "\n",
    "$v_1 = \\begin{pmatrix} 0.8 \\\\ 0.6 \\end{pmatrix}$ (normalized)\n",
    "\n",
    "$A A^T = \\begin{pmatrix} 25 & 50 \\\\ 50 & 100 \\end{pmatrix}$\n",
    "\n",
    "Eigenvalues: same nonzero, plus zero.\n",
    "\n",
    "$u_1 = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ (adjust sign if needed)\n",
    "\n",
    "SVD:  \n",
    "\\[\n",
    "A = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 & -2 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 5\\sqrt{5} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0.8 & 0.6 \\\\ 0.6 & -0.8 \\end{pmatrix}^T\n",
    "\\]\n",
    "\n",
    "> **Note**: In lecture, signs may differ; verify $A = U \\Sigma V^T$.\n",
    "\n",
    "\n",
    "### Interactive SVD Explorer\n",
    "Run the code below to generate a random low-rank matrix and compute its SVD. Predict the singular values and rank, then check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def generate_low_rank_matrix(m=3, n=4, rank=2):\n",
    "    \"\"\"\n",
    "    Generate a random m x n matrix with specified rank.\n",
    "    \"\"\"\n",
    "    U = np.random.randn(m, rank)\n",
    "    V = np.random.randn(n, rank)\n",
    "    A = U @ V.T\n",
    "    return A\n",
    "\n",
    "def matrix_to_latex(M):\n",
    "    rows = [r\" & \".join([f\"{x:.2f}\" if abs(x) > 1e-10 else \"0\" for x in row]) for row in M]\n",
    "    return r\"\\begin{pmatrix} \" + r\" \\\\ \".join(rows) + r\"\\end{pmatrix}\"\n",
    "\n",
    "# Generate matrix\n",
    "A = generate_low_rank_matrix()\n",
    "\n",
    "# Compute SVD\n",
    "U_svd, S, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "Sigma = np.zeros((U_svd.shape[1], Vt.shape[0]))\n",
    "np.fill_diagonal(Sigma, S)\n",
    "\n",
    "# Display matrix\n",
    "markdown = f\"**Matrix A** (rank ~2):\\n$$\\n{matrix_to_latex(A)}\\n$$\\n\\n\"\n",
    "markdown += \"Predict:\\n- Singular values (σ).\\n- Rank.\\n- Bases for U and V.\\n\\n\"\n",
    "display(Markdown(markdown))\n",
    "\n",
    "# Reveal answers (students uncomment after predicting)\n",
    "# markdown = f\"**SVD**:\\n\"\n",
    "# markdown += f\"- Singular values: {np.round(S, 3).tolist()}\\n\"\n",
    "# markdown += f\"- Rank: {np.sum(S > 1e-10)}\\n\"\n",
    "# markdown += f\"- U:\\n$$\\n{matrix_to_latex(np.round(U_svd, 3))}\\n$$\\n\"\n",
    "# markdown += f\"- Σ:\\n$$\\n{matrix_to_latex(np.round(Sigma, 3))}\\n$$\\n\"\n",
    "# markdown += f\"- V^T:\\n$$\\n{matrix_to_latex(np.round(Vt, 3))}\\n$$\\n\\n\"\n",
    "# display(Markdown(markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01779f",
   "metadata": {},
   "source": [
    "## Lecture 30: Linear transformations and their matrices\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "    src=\"https://www.youtube.com/embed/Ts3o2I8_Mxc\"\n",
    "    frameborder=\"0\"\n",
    "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "\n",
    "A **linear transformation** $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ is a function between vector spaces that preserves vector addition and scalar multiplication.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Definition\n",
    "\n",
    "$T$ is **linear** if for all vectors $v, w$ and scalars $c$:\n",
    "\n",
    "\\[\n",
    "\\boxed{T(v + w) = T(v) + T(w)}, \\quad \\boxed{T(c v) = c T(v)}\n",
    "\\]\n",
    "\n",
    "Equivalently:\n",
    "\n",
    "\\[\n",
    "T(c v + d w) = c T(v) + d T(w)\n",
    "\\]\n",
    "\n",
    "Consequences:\n",
    "\n",
    "- $T(0) = 0$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Examples and Non-Examples\n",
    "\n",
    "| Transformation | Linear? | Reason |\n",
    "|---------------|---------|--------|\n",
    "| Projection onto a line in $\\mathbb{R}^2$ | Yes | Preserves addition & scaling |\n",
    "| Rotation by any angle | Yes | Distances and directions preserved in a linear way |\n",
    "| Translation: $T(v) = v + v_0$ ($v_0 \\neq 0$) | No | $T(0) = v_0 \\neq 0$ |\n",
    "| Length: $T(v) = \\|v\\|$ | No | $T(-2v) = 2\\|v\\| \\neq -2 T(v)$ |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Matrix Representation\n",
    "\n",
    "Every linear transformation $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ can be represented by a matrix $A$ ($m \\times n$) such that\n",
    "\n",
    "\\[\n",
    "\\boxed{T(v) = A v}\n",
    "\\]\n",
    "\n",
    "**Key idea**: To define $T$ completely, it suffices to know $T(e_1), \\dots, T(e_n)$ where $\\{e_1, \\dots, e_n\\}$ is the **standard basis** of $\\mathbb{R}^n$.\n",
    "\n",
    "The columns of $A$ are exactly these images:\n",
    "\n",
    "\\[\n",
    "A = \\begin{bmatrix} T(e_1) & T(e_2) & \\cdots & T(e_n) \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 4. General Bases\n",
    "\n",
    "Suppose we choose any basis $\\{v_1, \\dots, v_n\\}$ for the input space and $\\{w_1, \\dots, w_m\\}$ for the output space.\n",
    "\n",
    "Any vector $v = c_1 v_1 + \\cdots + c_n v_n$, so\n",
    "\n",
    "\\[\n",
    "T(v) = c_1 T(v_1) + \\cdots + c_n T(v_n)\n",
    "\\]\n",
    "\n",
    "Express each $T(v_j)$ in the output basis:\n",
    "\n",
    "\\[\n",
    "T(v_j) = a_{1j} w_1 + \\cdots + a_{mj} w_m\n",
    "\\]\n",
    "\n",
    "Then the matrix $A$ (with respect to these bases) has columns given by these coefficients.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Example: Projection onto a Line\n",
    "\n",
    "Project onto the line in direction $b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ in $\\mathbb{R}^2$.\n",
    "\n",
    "Choose input basis:\n",
    "\n",
    "- $v_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ (unit vector along line)\n",
    "- $v_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ (perpendicular)\n",
    "\n",
    "Same for output basis (standard orthonormal works too, but let's use same).\n",
    "\n",
    "Projection leaves component along $v_1$ unchanged, drops component along $v_2$:\n",
    "\n",
    "\\[\n",
    "T(v_1) = v_1, \\quad T(v_2) = 0\n",
    "\\]\n",
    "\n",
    "Matrix (in this basis):\n",
    "\n",
    "\\[\n",
    "A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Example: From $\\mathbb{R}^3$ to $\\mathbb{R}^2$\n",
    "\n",
    "Let $T\\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} x + y \\\\ y + z \\end{pmatrix}$.\n",
    "\n",
    "Matrix (standard bases):\n",
    "\n",
    "\\[\n",
    "A = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "Columns: $T(e_1) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $T(e_2) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $T(e_3) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Example: Differentiation (Infinite Dimensions → Finite)\n",
    "\n",
    "Consider polynomials of degree $\\leq 2$: basis $\\{1, x, x^2\\}$.\n",
    "\n",
    "$T(p) = p'(x)$.\n",
    "\n",
    "\\[\n",
    "T(1) = 0, \\quad T(x) = 1, \\quad T(x^2) = 2x\n",
    "\\]\n",
    "\n",
    "Output basis $\\{1, x\\}$.\n",
    "\n",
    "Coefficients:\n",
    "\n",
    "- $T(1) = 0 \\cdot 1 + 0 \\cdot x$\n",
    "- $T(x) = 1 \\cdot 1 + 0 \\cdot x$\n",
    "- $T(x^2) = 0 \\cdot 1 + 2 \\cdot x$\n",
    "\n",
    "Matrix (2×3):\n",
    "\n",
    "\\[\n",
    "A = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Visualizing a Linear Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a 2x2 matrix (linear transformation)\n",
    "A = np.array([[2, 1],\n",
    "              [0, 1.5]])  # Shear + scale\n",
    "\n",
    "# Grid of points in unit square\n",
    "x = np.linspace(0, 1, 10)\n",
    "y = np.linspace(0, 1, 10)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "points = np.stack([X.ravel(), Y.ravel()])\n",
    "\n",
    "# Apply transformation\n",
    "transformed = A @ points\n",
    "X_t = transformed[0].reshape(10, 10)\n",
    "Y_t = transformed[1].reshape(10, 10)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original\n",
    "ax1.plot([0,1,1,0,0], [0,0,1,1,0], 'b', linewidth=2)\n",
    "ax1.grid(True)\n",
    "ax1.set_xlim(-0.5, 1.5); ax1.set_ylim(-0.5, 1.5)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_title('Original Unit Square')\n",
    "\n",
    "# Transformed\n",
    "ax2.plot([0, A[:,0][0], A[:,0][0]+A[:,1][0], A[:,1][0], 0],\n",
    "         [0, A[:,0][1], A[:,0][1]+A[:,1][1], A[:,1][1], 0], 'r', linewidth=2)\n",
    "ax2.grid(True)\n",
    "ax2.set_xlim(-0.5, 3.5); ax2.set_ylim(-0.5, 3)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_title('After Transformation T(v) = A v')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac0a33",
   "metadata": {},
   "source": [
    "## Lecture 31: Change of basis; image compression\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "    src=\"https://www.youtube.com/embed/0h43aV4aH7I\"\n",
    "    frameborder=\"0\"\n",
    "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "\n",
    "## Lecture 31: Change of basis; image compression\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "    src=\"https://www.youtube.com/embed/0h43aV4aH7I\"\n",
    "    frameborder=\"0\"\n",
    "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "\n",
    "We explore how **changing the basis** can reveal structure in data and enable **compression** — representing the same information with fewer numbers.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Compression: Lossless vs Lossy\n",
    "\n",
    "- **Lossless**: Exact reconstruction (e.g., ZIP, PNG).\n",
    "- **Lossy**: Approximate reconstruction, but much higher compression (e.g., JPEG, MP3).\n",
    "\n",
    "A 512×512 grayscale image (8-bit pixels):  \n",
    "$n = 512^2 = 262\\,144$ numbers, each 0–255 → ~2 million bits.\n",
    "\n",
    "In the **standard pixel basis**, each basis vector has a 1 in one pixel and 0 elsewhere — highly inefficient for correlated images (e.g., a blackboard with smooth regions).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Change of Basis for Compression\n",
    "\n",
    "Idea: Represent the image $x \\in \\mathbb{R}^n$ in a new basis $\\{v_1, \\dots, v_n\\}$ where most coefficients are small or zero.\n",
    "\n",
    "Let $W$ be the matrix with columns $v_1, \\dots, v_n$ (orthonormal → $W^T W = I$).\n",
    "\n",
    "Then:\n",
    "\n",
    "\\[\n",
    "\\boxed{x = W c}, \\quad c = W^T x\n",
    "\\]\n",
    "\n",
    "- $c$: coefficients in new basis (lossless transform).\n",
    "- Compress by keeping only largest $|c_i|$ → $\\hat{c}$ (many zeros).\n",
    "- Reconstruct: $\\hat{x} = W \\hat{c}$.\n",
    "\n",
    "Good basis → few large coefficients → high compression with low error.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. JPEG: Discrete Cosine Transform (DCT)\n",
    "\n",
    "JPEG breaks image into 8×8 blocks (lossless tiling).\n",
    "\n",
    "Each block uses a **cosine basis** (Fourier-like, low to high frequencies).\n",
    "\n",
    "Low-frequency coefficients (smooth parts) are large; high-frequency (details) often small → quantized/thrown away (lossy).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Wavelets: An Alternative Basis\n",
    "\n",
    "Wavelets capture both location and frequency.\n",
    "\n",
    "Example basis for $\\mathbb{R}^8$ (Haar wavelets, scaled):\n",
    "\n",
    "- Constant: $[1,1,1,1,1,1,1,1]$\n",
    "- Coarse difference: $[1,1,1,1,-1,-1,-1,-1]$\n",
    "- Finer differences, etc.\n",
    "\n",
    "Advantages: Better for images with sharp edges.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Formal Change of Basis\n",
    "\n",
    "Suppose matrix $A$ represents a linear transformation in the **standard basis**.\n",
    "\n",
    "We change to new bases:\n",
    "\n",
    "- Input basis columns in matrix $M$ (standard → new input basis)\n",
    "- Output basis columns in matrix $N$ (standard → new output basis)\n",
    "\n",
    "Then the matrix in the new bases is:\n",
    "\n",
    "\\[\n",
    "\\boxed{B = N^{-1} A M}\n",
    "\\]\n",
    "\n",
    "If bases are the same ($M = N$), then $A$ and $B$ are **similar**:\n",
    "\n",
    "\\[\n",
    "\\boxed{B = M^{-1} A M}\n",
    "\\]\n",
    "\n",
    "Same eigenvalues, different eigenvectors.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Compression Pipeline\n",
    "\n",
    "\\[\n",
    "\\text{Image } x \n",
    "\\xrightarrow{\\text{change basis}} c = W^T x \n",
    "\\xrightarrow{\\text{quantize / threshold}} \\hat{c} \n",
    "\\xrightarrow{\\text{inverse}} \\hat{x} = W \\hat{c}\n",
    "\\]\n",
    "\n",
    "For orthonormal $W$: inverse is just $W^T$ → fast!\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Simple 1D Compression Demo (DCT-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fab6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import dct, idct\n",
    "\n",
    "# Synthetic smooth signal (highly compressible in DCT basis)\n",
    "n = 256\n",
    "x = np.sin(np.linspace(0, 4*np.pi, n)) + 0.5*np.cos(np.linspace(0, 20*np.pi, n))\n",
    "\n",
    "# DCT (orthonormal basis change)\n",
    "c = dct(x, norm='ortho')\n",
    "\n",
    "# Keep only top k coefficients\n",
    "k_values = [5, 20, 50, 256]\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(x, 'k', label='Original')\n",
    "plt.title('Original Signal')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.stem(c, use_line_collection=True)\n",
    "plt.title('DCT Coefficients')\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "for k in k_values:\n",
    "    c_hat = np.copy(c)\n",
    "    c_hat[k:] = 0\n",
    "    x_hat = idct(c_hat, norm='ortho')\n",
    "    compression_ratio = n / k\n",
    "    plt.plot(x_hat, label=f'k={k} (compression ~{compression_ratio:.0f}x)')\n",
    "\n",
    "plt.title('Reconstructions')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e322a",
   "metadata": {},
   "source": [
    "## Lecture 32: Left and right inverses; pseudoinverse\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "    src=\"https://www.youtube.com/embed/Go2aLo7ZOlU\"\n",
    "    frameborder=\"0\"\n",
    "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "\n",
    "Not every matrix has a two-sided inverse, but **every** matrix has a **pseudoinverse** — the closest thing to an inverse, built from the SVD.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Two-Sided Inverse\n",
    "\n",
    "$A$ ($n \\times n$) has an inverse $A^{-1}$ if\n",
    "\n",
    "\\[\n",
    "\\boxed{A A^{-1} = A^{-1} A = I}\n",
    "\\]\n",
    "\n",
    "Exists $\\iff$ full rank ($r = n$), null space $\\{0\\}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Left Inverse (Tall Matrix, Full Column Rank)\n",
    "\n",
    "$A$: $m \\times n$ with $m > n$ and $\\rank(A) = n$ (full column rank).\n",
    "\n",
    "- $N(A) = \\{0\\}$ → at most one solution to $Ax = b$.\n",
    "- $A^T A$ ($n \\times n$) is symmetric positive definite → invertible.\n",
    "\n",
    "**Left inverse**:\n",
    "\n",
    "\\[\n",
    "\\boxed{A_L^{-1} = (A^T A)^{-1} A^T}\n",
    "\\]\n",
    "\n",
    "Check:\n",
    "\n",
    "\\[\n",
    "A_L^{-1} A = (A^T A)^{-1} A^T A = I_n\n",
    "\\]\n",
    "\n",
    "Cannot have $A A_L^{-1} = I_m$ (wrong size).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Right Inverse (Wide Matrix, Full Row Rank)\n",
    "\n",
    "$A$: $m \\times n$ with $m < n$ and $\\rank(A) = m$ (full row rank).\n",
    "\n",
    "- $N(A^T) = \\{0\\}$ → $b$ must be in $C(A)$ for consistency; then infinitely many solutions.\n",
    "- $A A^T$ ($m \\times m$) invertible.\n",
    "\n",
    "**Right inverse**:\n",
    "\n",
    "\\[\n",
    "\\boxed{A_R^{-1} = A^T (A A^T)^{-1}}\n",
    "\\]\n",
    "\n",
    "Check:\n",
    "\n",
    "\\[\n",
    "A A_R^{-1} = A A^T (A A^T)^{-1} = I_m\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Projections Revisited\n",
    "\n",
    "- Projection onto column space $C(A)$:\n",
    "\n",
    "\\[\n",
    "\\boxed{P = A (A^T A)^{-1} A^T}\n",
    "\\]\n",
    "\n",
    "  (If full column rank, this is the left inverse.)\n",
    "\n",
    "- Projection onto row space $C(A^T)$:\n",
    "\n",
    "\\[\n",
    "\\boxed{P = A^T (A A^T)^{-1} A}\n",
    "\\]\n",
    "\n",
    "  (If full row rank, this is the right inverse.)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Pseudoinverse (For Any Matrix)\n",
    "\n",
    "The **Moore-Penrose pseudoinverse** $A^+$ satisfies:\n",
    "\n",
    "- $A A^+ A = A$\n",
    "- $A^+ A A^+ = A^+$\n",
    "- $(A A^+)^T = A A^+$, $(A^+ A)^T = A^+ A$\n",
    "\n",
    "**Construction via SVD**:\n",
    "\n",
    "\\[\n",
    "A = U \\Sigma V^T, \\quad \\rank(A) = r\n",
    "\\]\n",
    "\n",
    "$\\Sigma$: $m \\times n$, nonzero singular values $\\sigma_1, \\dots, \\sigma_r$ on diagonal.\n",
    "\n",
    "Pseudoinverse of $\\Sigma$:\n",
    "\n",
    "\\[\n",
    "\\Sigma^+ : n \\times m, \\quad (\\Sigma^+)_{ii} = \n",
    "\\begin{cases}\n",
    "1/\\sigma_i & i \\leq r \\\\\n",
    "0 & i > r\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Then:\n",
    "\n",
    "\\[\n",
    "\\boxed{A^+ = V \\Sigma^+ U^T}\n",
    "\\]\n",
    "\n",
    "Properties:\n",
    "\n",
    "- If $A$ invertible → $A^+ = A^{-1}$\n",
    "- If full column rank → $A^+ = (A^T A)^{-1} A^T$ (left inverse)\n",
    "- If full row rank → $A^+ = A^T (A A^T)^{-1}$ (right inverse)\n",
    "\n",
    "Geometrically: $A^+$ maps from column space back to row space, inverting the nonzero singular directions.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Least-Squares and Minimum-Norm Solutions\n",
    "\n",
    "- Overdetermined ($m > n$): $A^+ b = \\hat{x}$ → least-squares solution ($\\min \\|Ax - b\\|$)\n",
    "- Underdetermined ($m < n$): $A^+ b = \\hat{x}$ → minimum-norm solution among all solutions\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Compute Pseudoinverse and Solve Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62fed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Example: tall matrix (full column rank)\n",
    "A_tall = np.array([[1, 0],\n",
    "                   [1, 1],\n",
    "                   [1, 2]], dtype=float)\n",
    "b_tall = np.array([0, 1, 3])\n",
    "\n",
    "# Example: wide matrix (full row rank)\n",
    "A_wide = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]], dtype=float)\n",
    "b_wide = np.array([1, 2])\n",
    "\n",
    "# Pseudoinverse via np.linalg.pinv (uses SVD)\n",
    "A_plus_tall = np.linalg.pinv(A_tall)\n",
    "A_plus_wide = np.linalg.pinv(A_wide)\n",
    "\n",
    "x_tall = A_plus_tall @ b_tall\n",
    "x_wide = A_plus_wide @ b_wide\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "**Tall matrix** (3×2, full column rank):\n",
    "\n",
    "$$ A = \\\\begin{{pmatrix}} 1 & 0 \\\\\\\\ 1 & 1 \\\\\\\\ 1 & 2 \\\\end{{pmatrix}} $$, \n",
    "$$ b = \\\\begin{{pmatrix}} 0 \\\\\\\\ 1 \\\\\\\\ 3 \\\\end{{pmatrix}} $$\n",
    "\n",
    "Least-squares solution: $$ \\\\hat{{x}} = {np.round(x_tall, 3).tolist()} $$\n",
    "\n",
    "Residual $$ \\\\|A\\\\hat{{x}} - b\\\\| \\\\approx {np.linalg.norm(A_tall @ x_tall - b_tall):.3f} $$\n",
    "\n",
    "**Wide matrix** (2×3, full row rank):\n",
    "\n",
    "$$ A = \\\\begin{{pmatrix}} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\end{{pmatrix}} $$, \n",
    "$$ b = \\\\begin{{pmatrix}} 1 \\\\\\\\ 2 \\\\end{{pmatrix}} $$\n",
    "\n",
    "Minimum-norm solution: $$ \\\\hat{{x}} = {np.round(x_wide, 3).tolist()} $$\n",
    "\n",
    "Norm $$ \\\\|\\\\hat{{x}}\\\\| \\\\approx {np.linalg.norm(x_wide):.3f} $$\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8623d",
   "metadata": {},
   "source": [
    "## Lecture 33: Recap\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "    src=\"https://www.youtube.com/embed/RWvi4Vx4CDc\"\n",
    "    frameborder=\"0\"\n",
    "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "\n",
    "No lecture notes or exercises for recap, but feel free to watch and review!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "source_map": [
   12,
   137,
   175,
   326,
   363,
   483,
   521,
   663,
   704
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}