{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb62d10",
   "metadata": {},
   "source": [
    "# Week 4: Orthogonal Basis and Projection\n",
    "\n",
    "## Lecture 13: Concept Review & Consolidation\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "    src=\"https://www.youtube.com/embed/l88D4r74gtM\"\n",
    "    frameborder=\"0\"\n",
    "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "\n",
    "We've include this for your interest and completeness, but feel free to skip if you are confident in your understanding so far; this is a skill/concept review lecture.\n",
    "\n",
    "## Lecture 14: Orthogonal Basis\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "    src=\"https://www.youtube.com/embed/YzZUIYRCE38\"\n",
    "    frameborder=\"0\"\n",
    "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "\n",
    "We introduce orthogonal vectors, orthogonal subspaces, and orthogonal bases, key for understanding geometric relationships in vector spaces and solving problems like projections. We connect these to the fundamental subspaces from Lecture 10.\n",
    "\n",
    "We show **why** the row space is orthogonal to the null space, and the column space is orthogonal to the left-null space.  \n",
    "These two *orthogonal-complement* pairs completely carve $\\mathbb{R}^n$ and $\\mathbb{R}^m$ into perpendicular pieces.\n",
    "\n",
    "### 1. Orthogonal Vectors – the dot-product test\n",
    "Two vectors $u, v \\in \\mathbb{R}^k$ are **orthogonal** if  \n",
    "$\n",
    "u^{T}v = 0 .\n",
    "$\n",
    "Geometrically they are perpendicular (90°).  \n",
    "The Pythagorean theorem holds **only** when the vectors are orthogonal:\n",
    "\n",
    "$$\n",
    "\\|u+v\\|^{2}= (u+v)^{T}(u+v)= u^{T}u + 2u^{T}v + v^{T}v .\n",
    "\\qquad \\text{If } u^{T}v=0 \\;\\Rightarrow\\; \\|u+v\\|^{2}= \\|u\\|^{2}+\\|v\\|^{2}.\n",
    "$$\n",
    "\n",
    "### 2. Orthogonal Subspaces\n",
    "Subspaces $S, T \\subset \\mathbb{R}^k$ are **orthogonal** if **every** vector in $S$ is orthogonal to **every** vector in $T$.  \n",
    "Notation: $S \\perp T$.\n",
    "\n",
    "### 3. The Four Subspaces are Orthogonal Pairs  \n",
    "\n",
    "| Pair (in $\\mathbb{R}^n$) | Pair (in $\\mathbb{R}^m$) |\n",
    "|--------------------------|--------------------------|\n",
    "| **Row space** $C(A^{T})$ $\\perp$ **Null space** $N(A)$ | **Column space** $C(A)$ $\\perp$ **Left-null space** $N(A^{T})$ |\n",
    "\n",
    "#### Why does $C(A^{T}) \\perp N(A)$?\n",
    "* $x \\in N(A)$ means $Ax = 0$.  \n",
    "* Examine product Ax as the dot product of each row vector in A with x:  \n",
    "  $\n",
    "  \\text{(row}_i \\text{ of }A) \\cdot x = 0 \\quad \\text{for every } i.\n",
    "  $\n",
    "* Every row of $A$ (i.e. every vector in the row space) is orthogonal to $x$.  \n",
    "* Because this holds for **all** $x\\in N(A)$, the whole row space is orthogonal to the whole null space.\n",
    "\n",
    "The same argument (applied to $A^{T}$) shows $C(A) \\perp N(A^{T})$.\n",
    "\n",
    "### 4. Orthogonal Complements  \n",
    "For a subspace $S\\subset \\mathbb{R}^k$ its **orthogonal complement** contains all the other orthogonal dimensions in the subspace:\n",
    "$\n",
    "\\dim S + \\dim S^{\\perp} = k\n",
    "$\n",
    "\n",
    "$\n",
    "S^{\\perp}= \\{ v\\in\\mathbb{R}^k \\mid v^{T}u=0 \\;\\forall u\\in S\\}\n",
    "$\n",
    "\n",
    "Below, we give you a sneak peek of a method to calculate the 4 subspaces using singular value decomposition! We take the dot product to demonstrate orthogonality. It should be 0 or very close to 0 (numerical stability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "\n",
    "# Example matrix A (not square)\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "], dtype=float)\n",
    "\n",
    "# --- Compute the four fundamental subspaces via SVD ---\n",
    "U, s, Vt = svd(A)\n",
    "r = np.sum(s > 1e-10)   # rank\n",
    "\n",
    "# Bases for each subspace\n",
    "row_space = Vt[:r].T              # C(A^T)\n",
    "null_space = Vt[r:].T             # N(A)\n",
    "col_space = U[:, :r]              # C(A)\n",
    "left_null_space = U[:, r:]        # N(A^T)\n",
    "\n",
    "# --- Check orthogonality numerically ---\n",
    "def check_orthogonal(X, Y, nameX, nameY):\n",
    "    dot_product = X.T @ Y\n",
    "    print(f\"{nameX} ⟂ {nameY}?  ||dot|| = {np.linalg.norm(dot_product):.2e}\")\n",
    "\n",
    "check_orthogonal(row_space, null_space, \"Row space\", \"Null space\")\n",
    "check_orthogonal(col_space, left_null_space, \"Column space\", \"Left-null space\")\n",
    "\n",
    "# --- Dimensional sanity checks ---\n",
    "print(\"\\nDimensional checks:\")\n",
    "print(f\"dim(Row space) + dim(Null space) = {row_space.shape[1]} + {null_space.shape[1]} = {A.shape[1]}\")\n",
    "print(f\"dim(Column space) + dim(Left-null space) = {col_space.shape[1]} + {left_null_space.shape[1]} = {A.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f157c",
   "metadata": {},
   "source": [
    "## Lecture 15: Projections onto subspaces\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "    src=\"https://www.youtube.com/embed/Y_Ac6KiQ1t0\"\n",
    "    frameborder=\"0\"\n",
    "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "\n",
    "We learn how to **find the closest point** in a subspace to a given vector $b$.  \n",
    "This is the foundation of **least squares** — solving $Ax = b$ when no exact solution exists.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Projecting a Vector onto a Line (2D Intuition)\n",
    "\n",
    "Suppose we have two vectors in $\\mathbb{R}^2$:  \n",
    "- $a$: defines a **line** through the origin  \n",
    "- $b$: any point we want to project\n",
    "\n",
    "**Goal**: Find the point $p$ on the line through $a$ that is **closest** to $b$.\n",
    "\n",
    "Let $p = x a$ (some scalar multiple of $a$).  \n",
    "The **error vector** is  \n",
    "$$\n",
    "e = b - p = b - x a\n",
    "$$\n",
    "\n",
    "For $p$ to be the *closest* point, $e$ must be **perpendicular** to the line (i.e., to $a$):  \n",
    "$$\n",
    "a^T e = 0 \\quad \\Rightarrow \\quad a^T (b - x a) = 0\n",
    "$$\n",
    "$$\n",
    "a^T b - x (a^T a) = 0 \\quad \\Rightarrow \\quad x = \\frac{a^T b}{a^T a}\n",
    "$$\n",
    "$$\n",
    "\\boxed{p = a \\left( \\frac{a^T b}{a^T a} \\right) = \\frac{a a^T}{a^T a} \\, b}\n",
    "$$\n",
    "\n",
    "Let  \n",
    "$$\n",
    "\\boxed{P = \\frac{a a^T}{a^T a}}\n",
    "\\qquad \\text{then} \\quad\n",
    "p = P b\n",
    "$$\n",
    "\n",
    "$P$ is the **projection matrix** onto the line through $a$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Properties of the Projection Matrix $P$\n",
    "\n",
    "| Property | Why it holds |\n",
    "|--------|--------------|\n",
    "| $P^2 = P$ | Projecting twice gives the same result |\n",
    "| $P^T = P$ | Symmetric (follows from $a a^T = (a a^T)^T$) |\n",
    "| $\\text{rank}(P) = 1$ | Column space is the line spanned by $a$ |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. General Case: Projecting onto a Subspace $C(A)$\n",
    "\n",
    "Now let $A$ be an $m \\times n$ matrix (not necessarily square).  \n",
    "We want to project $b \\in \\mathbb{R}^m$ onto **$C(A)$** — the column space of $A$.\n",
    "\n",
    "Let  \n",
    "$$\n",
    "p = A \\hat{x} \\quad \\text{(so $p \\in C(A)$)}\n",
    "$$\n",
    "Error:  \n",
    "$$\n",
    "e = b - p = b - A \\hat{x}\n",
    "$$\n",
    "\n",
    "For $p$ to be the **closest point**, $e$ must be **perpendicular to every column of $A$**:  \n",
    "$$\n",
    "a_i^T e = 0 \\quad \\forall i \\quad \\Rightarrow \\quad A^T e = 0\n",
    "$$\n",
    "$$\n",
    "A^T (b - A \\hat{x}) = 0 \\quad \\Rightarrow \\quad A^T A \\hat{x} = A^T b\n",
    "$$\n",
    "\n",
    "If $A^T A$ is invertible (i.e., $A$ has **full column rank**),  \n",
    "$$\n",
    "\\boxed{\\hat{x} = (A^T A)^{-1} A^T b}\n",
    "$$\n",
    "$$\n",
    "\\boxed{p = A \\hat{x} = A (A^T A)^{-1} A^T b}\n",
    "$$\n",
    "\n",
    "Define the **projection matrix onto $C(A)$**:  \n",
    "$$\n",
    "\\boxed{P = A (A^T A)^{-1} A^T}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Key Properties (Same as Before!)\n",
    "\n",
    "- $P^2 = P$  \n",
    "- $P^T = P$  \n",
    "- $\\text{rank}(P) = \\text{rank}(A)$  \n",
    "- $Pb \\in C(A)$  \n",
    "- $b - Pb \\in N(A^T)$ (i.e., perpendicular to $C(A)$)\n",
    "\n",
    "> **Note**: $A$ is **not square** → $P \\neq I$, even if $A$ is invertible in some sense.  \n",
    "> Only when $C(A) = \\mathbb{R}^m$ (full row rank) do we get $P = I$.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Example: Project onto the $xy$-Plane\n",
    "\n",
    "Let  \n",
    "$$\n",
    "A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad\n",
    "b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Column space: the $xy$-plane in $\\mathbb{R}^3$.\n",
    "\n",
    "Compute:  \n",
    "$$\n",
    "A^T A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad\n",
    "(A^T A)^{-1} = I_2\n",
    "$$\n",
    "$$\n",
    "P = A (A^T A)^{-1} A^T = A A^T = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "p = P b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "Error:  \n",
    "$$\n",
    "e = b - p = \\begin{pmatrix} 0 \\\\ 0 \\\\ 3 \\end{pmatrix} \\in N(A^T)\n",
    "$$\n",
    "\n",
    "Check: $e \\perp C(A)$?  \n",
    "$$\n",
    "\\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\cdot e = 0, \\quad\n",
    "\\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\cdot e = 0\n",
    "\\;\\; \\checkmark\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Why Do We Care? → **Least Squares**\n",
    "\n",
    "The system $Ax = b$ may have **no solution** (if $b \\notin C(A)$).  \n",
    "Instead, solve the **closest problem**:  \n",
    "$$\n",
    "\\text{minimize } \\|Ax - b\\|^2\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\text{solution: } \\hat{x} = (A^T A)^{-1} A^T b\n",
    "$$\n",
    "\n",
    "This is **linear regression**, **data fitting**, **curve fitting**, etc.\n",
    "\n",
    "**Example**: Fit a line $y = c + dt$ to noisy points $(t_i, b_i)$  \n",
    "→ Let $A = \\begin{pmatrix} 1 & t_1 \\\\ 1 & t_2 \\\\ \\vdots \\end{pmatrix}$  \n",
    "→ Solve $A \\hat{x} = P b$ → best-fit line.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: The Big Picture\n",
    "\n",
    "| Concept | Formula | Meaning |\n",
    "|-------|--------|--------|\n",
    "| Projection onto line | $p = \\frac{a a^T}{a^T a} b$ | Shadow of $b$ on line |\n",
    "| Projection onto $C(A)$ | $p = A (A^T A)^{-1} A^T b$ | Shadow of $b$ on column space |\n",
    "| Error $e = b - p$ | $e \\in N(A^T)$ | Perpendicular to subspace |\n",
    "| Least Squares | $\\hat{x} = (A^T A)^{-1} A^T b$ | Best approximate solution |\n",
    "\n",
    "### Visualization: Projection on the plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf9168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "#  Lecture 15 – Projection onto a random plane (no widgets)\n",
    "# ------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "import matplotlib as mpl\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Generate a random plane (two independent columns) and point b\n",
    "# ------------------------------------------------------------------\n",
    "def generate_data(seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    # Two random vectors → span a plane\n",
    "    while True:\n",
    "        a1 = np.random.randn(3)\n",
    "        a2 = np.random.randn(3)\n",
    "        if np.linalg.matrix_rank(np.column_stack([a1, a2])) == 2:\n",
    "            break\n",
    "    A = np.column_stack([a1, a2])\n",
    "    A = A / np.linalg.norm(A, axis=0)          # normalize for nice scale\n",
    "    b = np.random.randn(3) + 3                 # point not in plane\n",
    "    return A, b\n",
    "\n",
    "# Change the seed below to get a new random example\n",
    "A, b = generate_data(seed=42)                  # ← edit this number!\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Compute projection p = A (A^T A)^(-1) A^T b\n",
    "# ------------------------------------------------------------------\n",
    "ATA_inv = np.linalg.inv(A.T @ A)\n",
    "P = A @ ATA_inv @ A.T\n",
    "p = P @ b\n",
    "e = b - p                                      # error vector\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Helper: 3D arrow for the error vector\n",
    "# ------------------------------------------------------------------\n",
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        super().__init__((0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "    def do_3d_projection(self, renderer=None):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        return np.min(zs)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Plot everything\n",
    "# ------------------------------------------------------------------\n",
    "fig = plt.figure(figsize=(11, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_title('Projection of $b$ onto a Random Plane', fontsize=14, pad=20)\n",
    "\n",
    "# --- Plane (semi-transparent mesh) ---\n",
    "s = np.linspace(-2.5, 2.5, 20)\n",
    "t = np.linspace(-2.5, 2.5, 20)\n",
    "S, T = np.meshgrid(s, t)\n",
    "X = S * A[0,0] + T * A[0,1]\n",
    "Y = S * A[1,0] + T * A[1,1]\n",
    "Z = S * A[2,0] + T * A[2,1]\n",
    "ax.plot_surface(X, Y, Z, color='lightblue', alpha=0.5, linewidth=0, antialiased=True)\n",
    "\n",
    "# --- Points ---\n",
    "ax.scatter(*b, color='red', s=100, label='$b$ (original point)', depthshade=False)\n",
    "ax.scatter(*p, color='blue', s=100, label='$p$ (projection)', depthshade=False)\n",
    "ax.scatter(0, 0, 0, color='gray', s=30, alpha=0.6)\n",
    "\n",
    "# --- Error vector e = b - p (dashed purple arrow) ---\n",
    "arrow = Arrow3D(\n",
    "    [b[0], p[0]], [b[1], p[1]], [b[2], p[2]],\n",
    "    mutation_scale=20, arrowstyle='-|>', color='purple', linewidth=2.5, linestyle='--'\n",
    ")\n",
    "ax.add_artist(arrow)\n",
    "\n",
    "# --- Dotted perpendicular line from b to p ---\n",
    "ax.plot([b[0], p[0]], [b[1], p[1]], [b[2], p[2]], 'k:', linewidth=1.8)\n",
    "\n",
    "# --- Basis vectors of the plane (thin gray) ---\n",
    "for i in range(2):\n",
    "    vec = 3.0 * A[:, i]\n",
    "    ax.quiver(0, 0, 0, vec[0], vec[1], vec[2], color='gray', alpha=0.7, linewidth=1.2)\n",
    "\n",
    "# --- Labels ---\n",
    "ax.text(b[0], b[1], b[2], \"  $b$\", color='red', fontsize=13, weight='bold')\n",
    "ax.text(p[0], p[1], p[2], \"  $p$\", color='blue', fontsize=13, weight='bold')\n",
    "mid = (b + p) / 2\n",
    "ax.text(mid[0], mid[1], mid[2], \"  $e$\", color='purple', fontsize=13, weight='bold')\n",
    "\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n",
    "ax.legend(loc='upper left', fontsize=11)\n",
    "ax.set_xlim([-4, 4]); ax.set_ylim([-4, 4]); ax.set_zlim([-4, 4])\n",
    "ax.view_init(elev=20, azim=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  Summary in Markdown (auto-updated when cell is re-run)\n",
    "# ------------------------------------------------------------------\n",
    "def latex_vec(v, name, dec=2):\n",
    "    entries = [f\"{x:.{dec}f}\" for x in v]\n",
    "    return f\"${name} = \\\\begin{{pmatrix}} {entries[0]} \\\\\\\\ {entries[1]} \\\\\\\\ {entries[2]} \\\\end{{pmatrix}}$\"\n",
    "\n",
    "summary = f\"\"\"\n",
    "### Projection Summary\n",
    "\n",
    "**Plane basis** (columns of $A$):  \n",
    "{latex_vec(A[:,0], 'a_1')}, {latex_vec(A[:,1], 'a_2')}\n",
    "\n",
    "**Point** $b$:  \n",
    "{latex_vec(b, 'b')}\n",
    "\n",
    "**Projection** $p = A (A^T A)^{-1} A^T b$:  \n",
    "{latex_vec(p, 'p')}\n",
    "\n",
    "**Error** $e = b - p$:  \n",
    "{latex_vec(e, 'e')}  \n",
    "$\\|e\\| = {np.linalg.norm(e):.3f}$\n",
    "\n",
    "**Orthogonality check** (should be ≈0):  \n",
    "$a_1^T e = {A[:,0]@e:.2e}$,  \n",
    "$a_2^T e = {A[:,1]@e:.2e}$\n",
    "\n",
    "---\n",
    "\n",
    "*To see a new random example, just **re-run this cell** (or change the `seed` value above).*\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0be1f",
   "metadata": {},
   "source": [
    "## Lecture 16: Projection Matrices\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "    src=\"https://www.youtube.com/embed/osh80YCg_GM\"\n",
    "    frameborder=\"0\"\n",
    "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "\n",
    "In this lecture we explore projection matrices and least-squares problems, focusing on projecting vectors onto subspaces and finding approximate solutions to $Ax = b$. It builds on Lecture 15’s projection concepts, and we can start to connect these concepts in linear algebra back to model fitting and optimization.\n",
    "\n",
    "We now **use** the projection matrix to solve real problems:  \n",
    "**When $Ax = b$ has no solution, find the best possible $\\hat{x}$.**\n",
    "\n",
    "### 1. Recall: The Projection Matrix\n",
    "\n",
    "For any $m \\times n$ matrix $A$ with **independent columns**,  \n",
    "$\n",
    "\\boxed{P = A (A^T A)^{-1} A^T}\n",
    "$\n",
    "projects any $b \\in \\mathbb{R}^m$ onto $C(A)$.\n",
    "\n",
    "| Case | Result |\n",
    "|------|--------|\n",
    "| $b \\in C(A)$ | $Pb = b$ |\n",
    "| $b \\perp C(A)$ | $Pb = 0$ |\n",
    "| Otherwise | $Pb = p \\in C(A)$: closest point to $b$ |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Least Squares: Minimize the Error\n",
    "\n",
    "When $b \\notin C(A)$, $Ax = b$ has **no solution**.  \n",
    "Instead, minimize the **squared error**:  \n",
    "$\n",
    "\\min_x \\|Ax - b\\|^2 = \\min_x \\|e\\|^2\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\text{solve } A^T A \\hat{x} = A^T b\n",
    "$\n",
    "\n",
    "If $A$ has independent columns, $A^T A$ is **invertible**, so  \n",
    "$\n",
    "\\boxed{\\hat{x} = (A^T A)^{-1} A^T b}, \\quad\n",
    "p = A \\hat{x} = P b\n",
    "$\n",
    "\n",
    "> **Key geometric fact**:  \n",
    "> $e = b - p \\perp C(A)$ → $A^T e = 0$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Example: Fitting a Line to 3 Points\n",
    "\n",
    "We want to fit $y = C + D t$ to data:  \n",
    "$\n",
    "(t_1, b_1) = (1,1), \\quad (2,2), \\quad (3,1)\n",
    "$\n",
    "\n",
    "Form the system:  \n",
    "$\n",
    "A = \\begin{pmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 2 \\\\\n",
    "1 & 3\n",
    "\\end{pmatrix}, \\quad\n",
    "b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}\n",
    "$\n",
    "\n",
    "No exact solution — but we can find **best fit**.\n",
    "\n",
    "Compute:  \n",
    "$\n",
    "A^T A = \\begin{pmatrix} 1&1&1 \\\\ 1&2&3 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1&1 \\\\ 1&2 \\\\ 1&3 \\end{pmatrix}\n",
    "= \\begin{pmatrix} 3 & 6 \\\\ 6 & 14 \\end{pmatrix}\n",
    "$\n",
    "$\n",
    "A^T b = \\begin{pmatrix} 1&1&1 \\\\ 1&2&3 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}\n",
    "= \\begin{pmatrix} 4 \\\\ 9 \\end{pmatrix}\n",
    "$\n",
    "\n",
    "Solve:  \n",
    "$\n",
    "\\begin{pmatrix} 3 & 6 \\\\ 6 & 14 \\end{pmatrix} \\hat{x} = \\begin{pmatrix} 4 \\\\ 9 \\end{pmatrix}\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\hat{x} = \\begin{pmatrix} \\hat{C} \\\\ \\hat{D} \\end{pmatrix}\n",
    "= \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{2} \\end{pmatrix}\n",
    "$\n",
    "\n",
    "**Best-fit line**:  \n",
    "$\n",
    "\\boxed{y = \\frac{2}{3} + \\frac{1}{2} t}\n",
    "$\n",
    "\n",
    "### 4. Verify: Projection $p = A \\hat{x}$\n",
    "\n",
    "$\n",
    "p = A \\hat{x} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{pmatrix}\n",
    "\\begin{pmatrix} 2/3 \\\\ 1/2 \\end{pmatrix}\n",
    "= \\begin{pmatrix} 2/3 + 1/2 \\\\ 2/3 + 1 \\\\ 2/3 + 3/2 \\end{pmatrix}\n",
    "= \\begin{pmatrix} 7/6 \\\\ 4/3 \\\\ 13/6 \\end{pmatrix}\n",
    "$\n",
    "\n",
    "Error:  \n",
    "$\n",
    "e = b - p = \\begin{pmatrix} 1 - 7/6 \\\\ 2 - 4/3 \\\\ 1 - 13/6 \\end{pmatrix}\n",
    "= \\begin{pmatrix} -1/6 \\\\ 2/3 \\\\ -7/6 \\end{pmatrix}\n",
    "$\n",
    "\n",
    "Check $e \\perp C(A)$:  \n",
    "$\n",
    "A^T e = \\begin{pmatrix} 1&1&1 \\\\ 1&2&3 \\end{pmatrix}\n",
    "\\begin{pmatrix} -1/6 \\\\ 2/3 \\\\ -7/6 \\end{pmatrix}\n",
    "= \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\quad \\checkmark\n",
    "$\n",
    "\n",
    "### 5. Why Is $A^T A$ Invertible?\n",
    "\n",
    "**Only if the columns of $A$ are linearly independent.**\n",
    "\n",
    "**Proof**:  \n",
    "Suppose $A^T A x = 0$. Then  \n",
    "$\n",
    "x^T (A^T A x) = 0 \\quad \\Rightarrow \\quad (Ax)^T (Ax) = 0 \\quad \\Rightarrow \\quad \\|Ax\\| = 0 \\quad \\Rightarrow \\quad Ax = 0\n",
    "$\n",
    "If columns are independent → $N(A) = \\{0\\}$ → $x = 0$.  \n",
    "Thus $N(A^T A) = \\{0\\}$ → $A^T A$ is **invertible**.\n",
    "\n",
    "> **Insight**:  \n",
    "> Perpendicular (or orthonormal) columns make $A^T A = I$ — the **\"best\"** case — but **any independent columns work**.\n",
    "\n",
    "### 6. Summary: Least Squares in One Picture\n",
    "\n",
    "| Step | Formula |\n",
    "|------|--------|\n",
    "| Model | $Ax = b$ |\n",
    "| No solution? | Minimize $\\|Ax - b\\|^2$ |\n",
    "| Normal equations | $A^T A \\hat{x} = A^T b$ |\n",
    "| Solution | $\\hat{x} = (A^T A)^{-1} A^T b$ |\n",
    "| Projection | $p = A \\hat{x} = P b$ |\n",
    "| Error | $e = b - p \\perp C(A)$ |\n",
    "\n",
    "We provide visualization and practice problem below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def generate_matrix_and_b():\n",
    "    \"\"\"\n",
    "    Generate a random 3x2 matrix with rank 2 and a random b.\n",
    "    \"\"\"\n",
    "    v1 = np.random.randint(-5, 6, 3)\n",
    "    v2 = np.random.randint(-5, 6, 3)\n",
    "    while np.linalg.matrix_rank(np.column_stack((v1, v2))) != 2:\n",
    "        v2 = np.random.randint(-5, 6, 3)\n",
    "    A = np.column_stack((v1, v2))\n",
    "    b = np.random.randint(-5, 6, 3)\n",
    "    return A, b\n",
    "\n",
    "def compute_projection(A, b):\n",
    "    \"\"\"\n",
    "    Compute projection matrix, projection of b onto C(A), and least-squares solution.\n",
    "    \"\"\"\n",
    "    P = np.dot(A, np.dot(np.linalg.inv(np.dot(A.T, A)), A.T))\n",
    "    x_hat = np.dot(np.linalg.inv(np.dot(A.T, A)), np.dot(A.T, b))\n",
    "    p = np.dot(A, x_hat)\n",
    "    e = b - p\n",
    "    return P, x_hat, p, e\n",
    "\n",
    "def matrix_to_latex(M):\n",
    "    rows = [r\" & \".join([f\"{x:.2f}\" if abs(x) > 1e-10 else \"0\" for x in row]) for row in M]\n",
    "    return r\"\\begin{pmatrix} \" + r\" \\\\ \".join(rows) + r\"\\end{pmatrix}\"\n",
    "\n",
    "def plot_projection(A, b, p, e):\n",
    "    \"\"\"\n",
    "    Plot b, p, and e in 3D.\n",
    "    \"\"\"\n",
    "    Q = np.linalg.qr(A)[0]  # Orthonormal basis for C(A)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot origin\n",
    "    ax.scatter([0], [0], [0], color='black', s=50, label='Origin')\n",
    "    \n",
    "    # Plot b, p, e as vectors\n",
    "    ax.quiver(0, 0, 0, b[0], b[1], b[2], color='blue', label='b', linewidth=2)\n",
    "    ax.quiver(0, 0, 0, p[0], p[1], p[2], color='green', label='Projection p', linewidth=2)\n",
    "    ax.quiver(p[0], p[1], p[2], e[0], e[1], e[2], color='red', label='Error e', linewidth=2)\n",
    "    \n",
    "    # Plot column space plane\n",
    "    x = np.linspace(-1, 1, 20)\n",
    "    y = np.linspace(-1, 1, 20)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            point = X[i, j] * Q[:, 0] + Y[i, j] * Q[:, 1]\n",
    "            Z[i, j] = point[2]\n",
    "            X[i, j] = point[0]\n",
    "            Y[i, j] = point[1]\n",
    "    ax.plot_surface(X, Y, Z, color='cyan', alpha=0.3, label='C(A)')\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    max_range = np.max(np.abs(np.concatenate([b, p, e]))) * 1.5\n",
    "    ax.set_xlim(-max_range, max_range)\n",
    "    ax.set_ylim(-max_range, max_range)\n",
    "    ax.set_zlim(-max_range, max_range)\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.title('Least-Squares Projection of b onto C(A)')\n",
    "    plt.show()\n",
    "\n",
    "# Generate matrix and vector\n",
    "A, b = generate_matrix_and_b()\n",
    "P, x_hat, p, e = compute_projection(A, b)\n",
    "\n",
    "# Verify properties\n",
    "symmetric = np.allclose(P, P.T)\n",
    "idempotent = np.allclose(np.dot(P, P), P)\n",
    "ortho_check = np.allclose(np.dot(A.T, e), 0)\n",
    "\n",
    "# Display results\n",
    "markdown = f\"**Matrix A**:\\n$$\\n{matrix_to_latex(A)}\\n$$\\n\\n\"\n",
    "markdown += f\"**Vector b**:\\n$$\\n{matrix_to_latex(b.reshape(-1, 1))}\\n$$\\n\\n\"\n",
    "display(Markdown(markdown))\n",
    "plot_projection(A, b, p, e)\n",
    "markdown = f\"Predict:\\n- Least-squares solution x̂.\\n- Projection p = A x̂.\\n- Error e = b - p.\\n- Is P symmetric and idempotent?\\n- Is e ⊥ C(A)?\\n\\n\"\n",
    "display(Markdown(markdown))\n",
    "\n",
    "# Reveal answers (students uncomment after predicting)\n",
    "# markdown = f\"**Answers**:\\n\"\n",
    "# markdown += f\"- Least-squares solution x̂:\\n$$\\n{matrix_to_latex(x_hat.reshape(-1, 1))}\\n$$\\n\"\n",
    "# markdown += f\"- Projection p:\\n$$\\n{matrix_to_latex(p.reshape(-1, 1))}\\n$$\\n\"\n",
    "# markdown += f\"- Error e:\\n$$\\n{matrix_to_latex(e.reshape(-1, 1))}\\n$$\\n\"\n",
    "# markdown += f\"- P is symmetric: {'True' if symmetric else 'False'}.\\n\"\n",
    "# markdown += f\"- P is idempotent: {'True' if idempotent else 'False'}.\\n\"\n",
    "# markdown += f\"- e ⊥ C(A): {'True' if ortho_check else 'False'}.\\n\\n\"\n",
    "# display(Markdown(markdown))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "source_map": [
   12,
   87,
   119,
   295,
   429,
   576
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}